Training with a single process on 1 GPUs.
Model swin_base_patch4_window7_224 created, param count: 86742203
Data processing configuration for current model + dataset:
	input_size: (1, 224, 224)
	interpolation: bicubic
	mean: (0.5128,)
	std: (0.2236,)
	crop_pct: 0.875
Using native Torch AMP. Training in mixed precision.
Scheduled epochs: 210
Traceback (most recent call last):
  File "train.py", line 846, in <module>
    main()
  File "train.py", line 634, in main
    train_metrics = train_one_epoch(
  File "train.py", line 706, in train_one_epoch
    loss = loss_fn(output, target)
  File "/home/zc1995/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zc1995/tokenlabeling_covid/loss/cross_entropy.py", line 48, in forward
    loss_cls = loss_fn(x[0], target[:, 0])
  File "/home/zc1995/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zc1995/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py", line 1120, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File "/home/zc1995/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 2824, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
